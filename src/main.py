import os
import re
import numpy as np
import pandas as pd
import pickle
import json
import random
import threading
import getopt
import sys
from scipy import sparse;
########################
with open("config/data_params.json", "r") as read_file:
    params = json.load(read_file)
mal_fp = params["mal_fp"]
benign_fp = params["benign_fp"]
mal_fp_test = params["mal_fp_test_loc"]
benign_fp_test = params["benign_fp_test_loc"]
limiter = params["limiter"] # if set to false the pipeline will process every app in dir, else process a set amount
lim_mal = params["lim_mal"] #limits mal apps parsed
lim_benign = params["lim_benign"] #limits benign apps parsed
########################
from src.data_creation.get_data import *
from src.data_creation.json_functions import *
from src.data_creation.dict_builder import *
from src.model_file.model_build import *
from src.data_creation.build_mat import *
########################
#for reading command line args for testing
argumentList = sys.argv[1:]
options = "t"
long_options = ["test","Test"]
arguments, values = getopt.getopt(argumentList, options, long_options)
for currentArgument, currentValue in arguments:
    if currentArgument in ("-t", "--test","--Test"):
        print ("--- RUNNING FOR TEST DATA ---")
        limiter = False
        mal_fp = mal_fp_test
        benign_fp = benign_fp_test
        print("Pulling Benign test data from: " + benign_fp_test)
        print("Pulling Malware test data from: " + mal_fp_test)
    else:
        continue
########################                     
print("--- Starting Maleware Detection Pipeline ---")
start = time.time()
if limiter == False:
    print()
    mal_app_names = [name for name in os.listdir(mal_fp) if os.path.isdir(mal_fp + "/" + name)]
    mal_app_names.remove('.ipynb_checkpoints')
    benign_app_names = [name for name in os.listdir(benign_fp) if os.path.isdir(benign_fp + "/" + name)]
    benign_app_names.remove('.ipynb_checkpoints')
else:
    print("Limiting app intake to " + str(lim_mal + lim_benign) + " apps")
    print()
    mal_app_names = [name for name in os.listdir(mal_fp) if os.path.isdir(mal_fp + "/" + name)]
    random.shuffle(mal_app_names) #randomize the list
    mal_app_names = mal_app_names[:lim_mal]
    
    benign_app_names = [name for name in os.listdir(benign_fp) if os.path.isdir(benign_fp + "/" + name)]
    random.shuffle(benign_app_names) #randomize the list
    benign_app_names = benign_app_names[:lim_benign]  
########################
s_app = time.time()
confirm_exc = create_app_files(benign_fp, benign_app_names,mal_fp, mal_app_names)
if confirm_exc:
    print("--- All Apps Parsed in " + str(int(time.time() - s_app)) + " Seconds ---")
    print()
    print()
else:
    raise ValueError("ERROR create_app_files failed")
########################
print("--- Starting Dictionary Creation ---")
print()
dst = time.time()
dict_B, dict_P, dict_I, dict_A, dict_B_test, dict_P_test, dict_I_test, dict_A_test = fast_dict()
for t,fname in zip([dict_A, dict_B, dict_P, dict_I],["dict_A", "dict_B", "dict_P", "dict_I"]):
    save_json(t,"src/matrices/dictionary/"+fname)                  
print("--- Dictionary Creation Done in " + str(int(time.time() - dst)) + " Seconds ---")
print()
print()
########################
print("--- Creating Sparse Matricies ---")
bege = time.time()
columns = []
columns_test = []
Arows_train = []
Arows_test = []

for i in dict_A.keys():
    Arows_train.append(i)
    for j in dict_A[i]:
        if j in columns:
            continue
        else:
            columns.append(j)           
for k in dict_A_test.keys():
    Arows_test.append(k)
    for j in dict_A_test[k]:
        if j in columns:
            continue
        else:
            columns.append(j)
            
with open("config/parsing_data.json", "r") as read_file:
    params = json.load(read_file)
multi_threading = params["multithreading"] 
columns.sort()
if multi_threading == True:           
    #MAT A SPARSE  
    t1 = threading.Thread(target=unpack_A, args=(dict_A,columns,Arows_train,)) 
    #MAT B SPARSE
    t2 = threading.Thread(target=unpack_B, args=(dict_B,columns,)) 
    #MAT P SPARSE
    t3 = threading.Thread(target=unpack_P, args=(dict_P,columns,)) 
    #MAT I SPARSE
    t4 = threading.Thread(target=unpack_I, args=(dict_I,columns,))
    #test set
    t5 = threading.Thread(target=unpack_A, args=(dict_A_test,columns, Arows_test,"Test",)) 
    #MAT B SPARSE
    t6 = threading.Thread(target=unpack_B, args=(dict_B_test,columns,"Test",)) 
    #MAT P SPARSE
    t7 = threading.Thread(target=unpack_P, args=(dict_P_test,columns,"Test",)) 
    #MAT I SPARSE
    t8 = threading.Thread(target=unpack_I, args=(dict_I_test,columns,"Test",))
    t1.start() 
    t2.start() 
    t3.start() 
    t4.start()
    t5.start() 
    t6.start() 
    t7.start() 
    t8.start()
    t1.join() 
    t2.join() 
    t3.join() 
    t4.join()
    t5.join() 
    t6.join() 
    t7.join() 
    t8.join()
else:
    unpack_A(dict_A,columns,Arows_train)
    unpack_B(dict_B,columns)
    unpack_P(dict_P,columns)
    unpack_I(dict_I,columns)
    unpack_A(dict_A_test,columns,Arows_test,"Test")
    unpack_B(dict_B_test,columns,"Test")
    unpack_P(dict_P_test,columns,"Test")
    unpack_I(dict_I_test,columns,"Test")
print("--- Done Creating Sparse Matricies in " + str(int(time.time() - bege)) + " Seconds ---")
print()
########################
sparse_A = sparse.load_npz('src/matrices/Train_A.npz')
sparse_B = sparse.load_npz('src/matrices/Train_B.npz')
sparse_P = sparse.load_npz('src/matrices/Train_P.npz')
sparse_I = sparse.load_npz('src/matrices/Train_I.npz')
sparse_A_test = sparse.load_npz('src/matrices/Test_A.npz')
sparse_B_test = sparse.load_npz('src/matrices/Test_B.npz')
sparse_P_test = sparse.load_npz('src/matrices/Test_P.npz')
sparse_I_test = sparse.load_npz('src/matrices/Test_I.npz')
svm_modeler(sparse_A,sparse_B,sparse_P,sparse_I, Arows_train,sparse_A_test,sparse_B_test,sparse_P_test,sparse_I_test,Arows_test)
print()
print()
print("--- HinDroid Done in " + str(int(time.time() - start)) + " Seconds ---")
print("---       Output to Results/ directory       ----")
########################
