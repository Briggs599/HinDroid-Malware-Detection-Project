import os
import re
import numpy as np
import pandas as pd
import pickle
import json
import random
import threading
import getopt
import sys
from scipy import sparse
########################
# with open("config/data_params.json", "r") as read_file:
#     params = json.load(read_file)
# mal_fp = params["mal_fp"]
# benign_fp = params["benign_fp"]
# mal_fp_test = params["mal_fp_test_loc"]
# benign_fp_test = params["benign_fp_test_loc"]
# limiter = params["limiter"] # if set to false the pipeline will process every app in dir, else process a set amount
# lim_mal = params["lim_mal"] #limits mal apps parsed
# lim_benign = params["lim_benign"] #limits benign apps parsed
# ########################
from src.data_creation.get_data import *
from src.data_creation.json_functions import *
from src.data_creation.dict_builder import *
#from src.model_file.model_build import *
from src.data_creation.build_mat import *
from src.node2vec.node2vec import *
# ########################
# #for reading command line args for testing
# argumentList = sys.argv[1:]
# options = "t"
# long_options = ["test","Test"]
# arguments, values = getopt.getopt(argumentList, options, long_options)
# for currentArgument, currentValue in arguments:
#     if currentArgument in ("-t", "--test","--Test"):
#         print ("--- RUNNING FOR TEST DATA ---")
#         limiter = False
#         mal_fp = mal_fp_test
#         benign_fp = benign_fp_test
#         print("Pulling Benign test data from: " + benign_fp_test)
#         print("Pulling Malware test data from: " + mal_fp_test)
#     else:
#         continue
# ########################                     
# print("--- Starting Maleware Detection Pipeline ---")
# start = time.time()
# if limiter == False:
#     print()
#     mal_app_names = [name for name in os.listdir(mal_fp) if os.path.isdir(mal_fp + "/" + name)]
#     mal_app_names = [family +"_"+name for family in mal_app_names for name in os.listdir(mal_fp+"/"+family) if os.path.isdir(mal_fp +"/"+ family + "/" + name)]
#     benign_app_names = [name for name in os.listdir(benign_fp) if os.path.isdir(benign_fp + "/" + name)]
# else:
#     print("Limiting app intake to " + str(lim_mal + lim_benign) + " apps")
#     print()
#     mal_app_names = [name for name in os.listdir(mal_fp) if os.path.isdir(mal_fp + "/" + name)]
#     mal_app_names_full = [family +"_"+name for family in mal_app_names for name in os.listdir(mal_fp+"/"+family) if os.path.isdir(mal_fp +"/"+ family + "/" + name)]
#     random.shuffle(mal_app_names_full) #randomize the list
#     mal_app_names = mal_app_names_full[:lim_mal]
#     benign_app_names = [name for name in os.listdir(benign_fp) if os.path.isdir(benign_fp + "/" + name)]
#     random.shuffle(benign_app_names) #randomize the list
#     benign_app_names = benign_app_names[:lim_benign]  
# ########################
# s_app = time.time()
# confirm_exc = create_app_files(benign_fp, benign_app_names,mal_fp, mal_app_names)
# if confirm_exc:
#     print("--- All Apps Parsed in " + str(int(time.time() - s_app)) + " Seconds ---")
#     print()
#     print()
# else:
#     raise ValueError("ERROR create_app_files failed")
# ########################
# print("--- Starting Dictionary Creation ---")
# dst = time.time()
# dict_B, dict_P, dict_I, dict_A, dict_A_test = fast_dict()
# for t,fname in zip([dict_A, dict_B, dict_P, dict_I, dict_A_test],["dict_A", "dict_B", "dict_P", "dict_I","dict_A_test"]):
#     save_json(t,"src/matrices/dictionary/"+fname)       
# print("--- Dictionary Creation Done in " + str(int(time.time() - dst)) + " Seconds ---")
# print()
# print()
# ########################
# print("--- Creating Sparse Matricies ---")
# bege = time.time()
# columns = list(set(list(dict_B.keys())))
# Arows_train = [i for i in dict_A.keys()]
# Arows_test = [i for i in dict_A_test.keys()]
# with open("config/parsing_data.json", "r") as read_file:
#     params = json.load(read_file)
# multi_threading = params["multithreading"] 
# columns.sort()
# if multi_threading == True:           
#     #MAT A SPARSE  
#     t1 = threading.Thread(target=unpack_A, args=(dict_A,columns,Arows_train,)) 
#     #MAT B SPARSE
#     t2 = threading.Thread(target=unpack_B, args=(dict_B,columns,)) 
#     #MAT P SPARSE
#     t3 = threading.Thread(target=unpack_P, args=(dict_P,columns,)) 
#     #MAT I SPARSE
#     t4 = threading.Thread(target=unpack_I, args=(dict_I,columns,))
#     #test set
#     t5 = threading.Thread(target=unpack_A, args=(dict_A_test,columns, Arows_test,"Test",)) 
#     t1.start() 
#     t2.start() 
#     t3.start() 
#     t4.start()
#     t5.start() 
#     t1.join() 
#     t2.join() 
#     t3.join() 
#     t4.join()
#     t5.join() 
# else:
#     unpack_A(dict_A,columns,Arows_train)
#     unpack_B(dict_B,columns)
#     unpack_P(dict_P,columns)
#     unpack_I(dict_I,columns)
#     unpack_A(dict_A_test,columns,Arows_test,"Test")
# print("--- Done Creating Sparse Matricies in " + str(int(time.time() - bege)) + " Seconds ---")
# print()
# ########################
sparse_A = sparse.load_npz('src/matrices/A_mat.npz')
print(sparse_A.todense())
A_copy = sparse_A.T
A = sparse_A * A_copy
print(A.todense())
# sparse_B = sparse.load_npz('src/matrices/Train_B.npz')
# sparse_P = sparse.load_npz('src/matrices/Train_P.npz')
# sparse_I = sparse.load_npz('src/matrices/Train_I.npz')
# sparse_A_test = sparse.load_npz('src/matrices/Test_A.npz')

# svm_modeler(sparse_A,sparse_B,sparse_P,sparse_I, Arows_train,sparse_A_test,Arows_test)
# print("HinDroid Done in " + str(int(time.time() - start)) + " Seconds")
# print()
# print("--- Output to Results/ directory ----")
########################
# node2vec
G = matrix_to_graph(A)
print(G.edges())
preprocess_transition_probs(G)
walks = simulate_walks(G, num_walks=10, walk_length=80)
learn_embeddings(walks)
########################